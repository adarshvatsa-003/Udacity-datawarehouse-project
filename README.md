# Building a Data Warehouse in AWS
**Implementing a SQL DWH in Amazon's cloud with Amazon Redshift.**



## Introduction
### What is this project about?
Imagine you have just been hired as a data engineer in a new music streaming startup called Sparkify.
The company is constantly growing, and now, they want you to move their processes and data onto the cloud.
As their data engineer, you are tasked with building an ETL pipeline that:
* Extracts their data from a S3 Bucket
* Stages the data in Redshift.
* Transforms data into a set of dimensional tables using SQL statements.

This will help their analytics team to continue finding insights in what songs their users are listening to.

So, **in this project you will build an ETL pipeline for a database hosted in Redshift.**

### Project Datasets
The data we are going to use resides in an Amazon S3 bucket. Here are the links for each:
* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`

Log data json path: `s3://udacity-dend/log_json_path.json`

Alternatively, you can find all this data in the [data folder](data).  
 
#### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/).
Each file is in JSON format and contains metadata about a song and the artist of that song.

These files are partitioned by the first three letters of each song's track ID.
For example, here are filepaths to two files in this dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```
{
    "num_songs": 1,
    "artist_id": "ARJIE2Y1187B994AB7",
    "artist_latitude": null,
    "artist_longitude": null,
    "artist_location": "",
    "artist_name": "Line Renaud",
    "song_id": "SOUPIRU12A6D4FA1E1",
    "title": "Der Kleine Dompfaff",
    "duration": 152.92036,
    "year": 0
}
```
#### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim)
based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![Log data](images/log-data.png)

## Project Steps

### Decide a schema for Song Play Analysis
In order to simplify queries and enable fast aggregations, we are going to use the **Star Schema** using the song and event datasets.
These tables will consist on:

**1 Fact Table**

* **songplays** - records in event data associated with song plays i.e. records with page NextSong
    * _songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent_

**4 Dimension Tables**

* **users** - users in the app
    * _user_id, first_name, last_name, gender, level_
* **songs** - songs in music database
    * _song_id, title, artist_id, year, duration_
* **artists** - artists in music database
    * _artist_id, name, location, lattitude, longitude_
* **time** - timestamps of records in songplays broken down into specific units
    * _start_time, hour, day, week, month, year, weekday_

### Check scripts

* Support files: 
    * `sql_queries.py` defines the SQL statements used in the project, which will be imported into the script files.
    * `dwh.cfg` stores all the information required to connect to S3 and Amazon Redshift.
* Scripts (In order of intented execution):
    1. `create_table.py` will create empty staging, fact and dimension tables in Redshift.
    2. `etl.py` will load data from S3 into staging tables on Redshift and then process that data into your analytics tables on Redshift.
    3. `analytics.py` will help us verify that the data was successfully integrated into the cluster.

### Create a Redshift cluster

#### Create a Security Group
Here, you'll create a security group you will later use to authorize access to your Redshift cluster.

1. Go to your [Amazon EC2 console](https://console.aws.amazon.com/ec2) and under **Network and Security** in the left navigation pane, select Security Groups.
2. Choose the **Create Security Group** button.
3. Enter `redshift_security_group` for **Security group name**.
4. Enter "authorize redshift cluster access" for **Description**.
5. Select the **Inbound** tab under **Security group rules**.
6. Click on **Add Rule** and enter the following values:
    * **Type:** Custom TCP Rule.
    * **Protocol:** TCP.
    * **Port Range:** `5439`. (The default port for Amazon Redshift is this one).
    * **Source:** select Custom IP, then type `0.0.0.0/0` or use your **Current IP**.
        * **Important:** Using `0.0.0.0/0` is not recommended for anything other than demonstration purposes because it allows access from any computer on the internet. In a real environment, you would create inbound rules based on your own network settings.

#### Create an IAM Role
For the cluster to access the S3 bucket, we need to grant it the `AmazonS3ReadOnlyAccess` policy.
This can be achieved by creating a new IAM role called `myRedshiftRole` with that policy.
Follow instructions in [Creating an IAM role](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#create-iam-role) to proceed. 

#### Start the cluster
Follow ["Getting started with Amazon Redshift: Steps 1-3"](https://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-launch-sample-cluster.html) at AWS Documentation in order to create a new cluster.

* The cluster configuration should match the following:
    * **Cluster Identifier:** `redshift-cluster-1`
    * **Type of machine:** `dc2.large`
    * **Number of compute nodes:** 1
* For database configurations:
    * **Database name:** `dev`
    * **Database port:** `5439`
    * **Master username:** `awsuser` 
    * **Master user password:** `this_password_is_PRETTY_exposed_123`
* Make sure to add these additional configurations:
    * **Cluster permissions:** `myRedshiftRole`
    * **VPC security groups:** ``redshift_security_group``
    * **Publicly accessible:** Yes
    * **Enhanced VPC routing:** Disabled

* Leave the rest of the parameters as default.

Starting the cluster takes AWS around 5 minutes by the time this file was written.

**WARNING:** The cluster that you create will be live, and you will be charged the standard Amazon Redshift usage
fees for the cluster until you delete it. Make sure to delete your cluster each time you're finished working to
avoid large, unexpected costs for yourself. Also, you can set up  [Billing Alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html) to prevent this from happening.

### Add redshift database and IAM role to your cfg file
Once you have it, you must copy down your cluster endpoint, the ARN for `myRedshiftRole`, and put them in ``dwh.cfg``.

It should look as follows:
```
[CLUSTER]
HOST = (check dwg dile)
DB_NAME = dev
DB_USER = awsuser
DB_PASSWORD = (check dwg file)
DB_PORT = 5439

[IAM_ROLE]
ARN = (check dwg file)

[S3]
LOG_DATA = s3://udacity-dend/log-data
LOG_JSONPATH = s3://udacity-dend/log_json_path.json
SONG_DATA = s3://udacity-dend/song_data
```
It should look

### Create the skeleton of the Data Warehouse
In this step we will run `create_tables.py`, which will take around 10 seconds.

This script will create the staging tables and those needed for the star schema.

### Load data into the cluster
In this step we will run `etl.py`, which will take around 10 minutes.

First, the staging tables will be loaded with data extracted from S3. Then, the data in these staging tables
will be transformed into the Star Schema.

### Verify the content in the database
Finally, we will run `analytics.py`, which takes just around 5 seconds.
```bash
Running:
SELECT COUNT(*) FROM staging_events
8056

Running:
SELECT COUNT(*) FROM staging_songs
14896

Running:
SELECT COUNT(*) FROM songplays
333

Running:
SELECT COUNT(*) FROM users
104

Running:
SELECT COUNT(*) FROM songs
14896

Running:
SELECT COUNT(*) FROM artists
10025

Running:
SELECT COUNT(*) FROM time
333
``` 

This verifies that our ETL process was successful.